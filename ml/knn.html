<!doctype html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Introducing KNN: K Nearest Neighbors</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet"
        integrity="sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH" crossorigin="anonymous">

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link
        href="https://fonts.googleapis.com/css2?family=Libre+Baskerville:ital,wght@0,400;0,700;1,400&family=Montserrat:wght@700;800&display=swap"
        rel="stylesheet">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/themes/prism-tomorrow.min.css" rel="stylesheet">

    <link rel="stylesheet" href="style.css">
    <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
</head>

<body>
    <div class="container">
        <div class="row mt-5 justify-content-center">
            <div class="col-md-10 px-5 py-5">
                <h1 class="montserrat-bold pt-5">
                    Introducing KNN: K Nearest Neighbors
                </h1>

                <div class="blog-info my-4">
                    <p class="text-muted mb-1">Posted on Sep 25th, 2024 by Felipe An√≠bal</p>
                    <p class="text-muted mb-1">Last updated on Sep 25th, 2024</p>
                </div>

                <p>
                    K Nearest Neighbors or simply KNN is a family of classifier algorithms that is conceptually easy to understand and to implement and nonetheless may achieves great results. In this post we will see a quick intuition behind KNN, a more formal description of it, some examples and use cases and some data preparation techniques that might help us make a better classifier.
                </p>

                <h2>Gaining intuition</h2>

                <p>
                    A classifier is essentially an algorithm that assigns a class for a given input. An algorithm that decides wether to approve or deny a loan to a person is an example of a classifier. Given an input - the information about a person - it assigns that person a class - deserving credit or not deserving credit. This kind of the decision is usually made from data collected in the past, if many people with given characteristics received a loan and were able to repay it, the someone with similar characteristics should likely be able to repay the loan as well. Likewise, if many people with a given characteristic were not able to repay the loan, then someone in similar conditions might not be able to pay either.
                </p>

                <p>
                    The reasoning above is precisely what is behind KNN. The idea is that if we look at a new input, we can assign it the same label of its neighbors, that is, the same label as similar inputs!
                </p>

                <p>
                    The idea above should intuitively make sense, but it is quite challenging to translate into a mathematical or programable idea. What do we mean by "similar characteristics". How can we quantify it?
                </p>

                <h2>A more formal definition</h2>

                <p>
                    In order to translate the idea above into a more formal definition, we can use a mathematical concept from linear algebra. We can think about each input of N attributes as a N-dimensional vector, better yet, we can think about it as point in a N-dimensional space. That way, given a new point, we give it the same label as the most frequent label among its neighbors. We can for instance look at the 3 points that are closest to the new point, and give the new point the label that appears most frequently. That would be a 3-NN a 3 nearest neighbors classifier. We could look at the 30 nearest points, and that would be 30-NN. We can vary the value of K to adjust our classifier.
                </p>
                
                <p>
                    A problem that arises from thinking about the input as vector is that we can only calculate the distances between two numerical vectors. How could we measure the distance between to features like "Married" and "Single". This is a problem that we will address in a moment. But for now, let us try to implement the idea above mentioned in an example using only numerical inputs.
                </p>

                <h2>KNN hands-on: the Adult dataset</h2>

                <p>
                    The university of California Irvine has published a dataset with information about ~32000 people from the US 1994 Census. This information includes, the number of years a person has studied, their capital gain and capital loss, their age, the number of hours that person works a week and their income. The income only shows wether or not that person earns more than 50,000 dollars a year. If we are given information about a new person, except their income, can we guess if that person earns more than 50,000 dollars?
                </p>

                <p>
                    The code below uses the scikit-learn library to implement a KNN that tries to make this prediction in python. The data used for this example can be found at the <a href="https://archive.ics.uci.edu/dataset/2/adult">UCI machine learning repository</a>.
                </p>

                <div class="code-container">

                
                    <pre>
<code><span class="comment"># Import pandas library</span>
import pandas as pd

<span class="comment"># Read the training data and mark the "?" as missing data</span>
adult = pd.read_csv("train_data.csv", na_values="?")

<span class="comment"># Select only the columns with numerical values</span>
xAdult = adult[["age", "education.num", "capital.gain", "capital.loss", "hours.per.week"]]

yAdult = list(nadult["income"])

<span class="comment"># Import KNN from the scikit-learn library</span>
from sklearn.neighbors import KNeighborsClassifier

<span class="comment"># We will use K=30</span>
knn = KNeighborsClassifier(n_neighbors=30)

<span class="comment"># Use cross validation to have an idea of our model's accuracy</span>
from sklearn.model_selection import cross_val_score

scores = cross_val_score(knn, xAdult, yAdult, cv=10)
avg = sum(scores)/10</code>
</pre>
                </div>
                <p>
                    Using the model above in the test data (also available at the <a href="https://archive.ics.uci.edu/dataset/2/adult">UCI machine learning repository</a>) we got an accuracy of around 82%.
                </p>

                <h2>Dealing with categorical features</h2>

                <p>
                    We saw above that our classifier makes a correct guess around 82% of the time. Let's see if we can use more information from the dataset to improve our prediction. We will turn our categorical features - the ones that don't have numerical values - into numerical features. There are 2 very common ways of doing that: label encoding and one-hot encoding.
                </p>

                <p>
                    Label encoding consists of simply assigning a number two each value of a categorical feature in the order they appear. For example, if we look at the feature "relationship", we can attribute 0 to Husband, 1 to Wife, 2 to Own-child, 3 to Not-in-family, 4 to Other-relative and 5 to Unmarried. Now we can feed that information to our classifier. We can do the same to each one of our other categorical features and then feed them to our classifier.
                </p>

                <p>
                    Another strategy is to encode each value in a categorical feature as a new feature with binary values. That means we can create new columns in our dataset like Husband, Wife, Own-child, Not-in-family, Other-relative and Unmarried, each with values 1 if a person fits that category and 0 if it doesn't. 
                </p>

                <p>
                    The second strategy has the advantage of not creating arbitrary magnitude difference and order between the values. What does it mean, for instance, that "Other-relative" is twice "Own-child" as the label encoding implies? Or why is "Husband" further from "Other-relative" than from "Wife"? On the other hand, one-hot encoding can dramatically increase the size of our dataset. In our example, if we encode each categorical feature using one-hot encoding we go from around 15 columns to more than 100 columns.
                </p>

                <p>
                    Let's see if including categorical data using one-hot encoding we can improve the accuracy of our classifier.
                </p>

                <h2>KNN hands-on (part II):</h2>

                <div class="code-container">
<pre>
<code><span class="comment"># Pandas has a method called get_dummies to do one-hot encoding</span>
encodedAdult = pd.get_dummies(nadult, columns=["workclass", "education", "marital.status",
"relationship", "occupation", "race", "sex"], dummy_na=True)

<span class="comment"># For our classifier we will not look at "Id" and "native.country".
# You can change that and see if it improves accuracy!</span>
encodedAdultX = encodedAdult.drop(columns=["fnlwgt", "income", "Id", "native.country"])

<span class="comment"># Import KNN from the scikit-learn library</span>
from sklearn.neighbors import KNeighborsClassifier

<span class="comment"># We will use K=30</span>
knn = KNeighborsClassifier(n_neighbors=30)

<span class="comment"># Use cross validation to have an idea of our model's accuracy</span>
from sklearn.model_selection import cross_val_score

scores = cross_val_score(knn, xAdult, yAdult, cv=10)
avg = sum(scores)/10</code>
</pre>
                </div>
                
                <p>
                    With the model above, we get an accuracy of around 85% on the test data.
                </p>

                <h2>Conclusion</h2>

                <p>
                    KNN is a simple, but powerful algorithm for classification. We can couple it with data preparation strategies like one-hot encoding and Label encoding and use it to quickly build a classifier. One disadvantage worth mentioning of KNN over other algorithms like neural-networks is performance when making a prediction. In order to classify a new value the KNN classifier has to look at every single point already at the dataset and sort then according to their distance. That might take a very long time for big enough datasets.
                </p>

                <p>The complete code with the examples mentioned here is available at: <a href="https://github.com/FelipeAnibal/felipeanibal.github.io/tree/main/ml/implementations/knn">https://github.com/FelipeAnibal/felipeanibal.github.io/tree/main/ml/implementations/knn</a></p>
            </div>
        </div>
    </div>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js"
        integrity="sha384-YvpcrYf0tY3lHB60NNkmXc5s9fDVZLESaAA55NDzOxhy9GkcIdslK1eN7N6jIeHz"
        crossorigin="anonymous"></script>
</body>

</html>
